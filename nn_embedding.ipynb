{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd84c21-dcaf-49e0-9b30-ca238b72982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1724972b-95d7-423c-8346-54d300469da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 64 #num_embeddings = size of the dictionary of embeddings\n",
    "dim = 1024 #embedding_dim = size of each embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b74718-cc9a-4f52-b93a-906be8773a5b",
   "metadata": {},
   "source": [
    "an embedding is just a lookup table.\n",
    "\n",
    "when initializing, you create n_pos different indices, e.g., 1 for each word.\n",
    "then, each word will be represented by dim floats which are calculated based on the weights of the embedding.\n",
    "the weights are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d45608f-0222-4656-9020-1b835937d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nn.Embedding(n_pos, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d609b4-2ad4-452c-982d-68baf4feae9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1024]),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1920, -0.6611, -0.3977,  ..., -0.2789,  0.4131, -1.8262],\n",
       "         [ 0.4764,  0.1507,  1.2597,  ...,  2.0319,  0.0784, -0.5667],\n",
       "         [-0.8787,  1.3705,  0.3430,  ...,  1.0124, -0.8911,  1.4979],\n",
       "         ...,\n",
       "         [-0.2363, -1.3935, -0.3395,  ...,  0.0656, -0.8006,  0.6956],\n",
       "         [-0.3866, -1.0277, -0.1494,  ...,  0.3896,  0.6084,  1.9942],\n",
       "         [ 0.1433,  0.6445, -0.1725,  ...,  0.9806,  0.3934, -0.1202]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.weight.shape,e.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d94a6c-5b9f-4342-baa4-9a021739a8e5",
   "metadata": {},
   "source": [
    "to retrieve an embedding, you pass a list of indices (ints). for each of them, you will retrieve an embedding of size dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17deb9fe-c0cf-48f0-9395-e63189d9458f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 522, 1024]),\n",
       " tensor([[[ 0.3406,  0.9814,  1.0833,  ..., -1.2019,  1.0401,  0.5690],\n",
       "          [ 0.8973,  0.8471,  0.7570,  ...,  0.1485, -1.1242, -1.2132],\n",
       "          [-1.4073,  1.2259,  1.0179,  ..., -0.1685,  1.1933,  0.1820],\n",
       "          ...,\n",
       "          [-0.4392,  0.1687,  0.7692,  ..., -0.6926,  1.0820,  0.3284],\n",
       "          [ 0.0185, -0.0246,  0.5937,  ...,  0.1380, -1.4950,  0.8448],\n",
       "          [ 0.8973,  0.8471,  0.7570,  ...,  0.1485, -1.1242, -1.2132]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(low=14,high=43,size=(1,522))\n",
    "e(a).shape,e(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f79cb79f-5bbd-4406-855f-fc24ee1ae5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2378, 1024]),\n",
       " tensor([[[-0.4807, -1.0984, -1.1376,  ..., -0.1271,  1.0515, -1.1907],\n",
       "          [-0.7495,  1.5448,  1.7666,  ..., -0.3674,  2.5782,  0.4107],\n",
       "          [-0.4392,  0.1687,  0.7692,  ..., -0.6926,  1.0820,  0.3284],\n",
       "          ...,\n",
       "          [-0.8756, -0.8425,  0.6149,  ..., -0.6144,  0.7528, -0.3294],\n",
       "          [-2.5290, -0.3384,  0.3816,  ..., -0.6995, -1.0060,  0.3404],\n",
       "          [-1.1847,  0.2865,  0.0560,  ..., -1.4141, -1.4858,  0.6790]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(low=14,high=43,size=(1,2378))\n",
    "e(a).shape,e(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c2c03-b9ce-4e94-bb9c-7bedf0050597",
   "metadata": {},
   "source": [
    "it seems that the problem is that the image that I am trying to predict has position values that are higher than n_pos which is used to construct the embedding, i.e., my max integer in the input is 74 > 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1237afd-2581-4b97-840c-6f8e2bdc1f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m,size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2378\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43me\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape,e(a)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "a = torch.randint(low=60,high=70,size=(1,2378))\n",
    "e(a).shape,e(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc23540-0fbe-41b3-8507-bac134af3655",
   "metadata": {},
   "source": [
    "as you can see, it works if n_pos > max(input).\n",
    "the reason is that there are basically only 64 entries in the lookup table while you are querying for entries that do not exist.\n",
    "\n",
    "an analogy would be that you create a dictionary containing 64 words, but you would like to get an embedding of a word that is not in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a55c0033-1d1b-4088-b172-c2a4500d8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = nn.Embedding(80, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dbc9cc9-f619-4796-a85b-ef3689fd91c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2378, 1024]),\n",
       " tensor([[[ 1.0577, -0.9709,  0.9988,  ..., -0.4633,  0.0941,  1.2053],\n",
       "          [-0.4897,  1.1873, -0.1907,  ..., -0.9569, -2.1632, -1.5653],\n",
       "          [-0.7045,  2.2276, -2.4967,  ...,  1.4843, -0.4477, -1.1457],\n",
       "          ...,\n",
       "          [ 0.9064, -0.1055, -1.0278,  ...,  2.2449, -0.1209,  0.2769],\n",
       "          [-0.7045,  2.2276, -2.4967,  ...,  1.4843, -0.4477, -1.1457],\n",
       "          [-0.7045,  2.2276, -2.4967,  ...,  1.4843, -0.4477, -1.1457]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2(a).shape,e2(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
